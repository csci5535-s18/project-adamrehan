\section{Abstract}
In this project, we are proposing to solve algebraic word problems in a way similar to Kushman et al \citep{kushman2014}. However, rather than relying on a rule based approach \citep{MukherjeeandGarain}, or a mapping function to a set of templates (as in Kushman et al), we are proposing a slightly different method. The method that we are proposing is much closer to the solution in \citep{Hosseini2014}. In this method, we will learn a mapping from the input sequence of words, to the set of verbs and their syntactic dependents, that include some type of quantifier, via a dependency parse of the text. We will then categorize the verb expression to be equivalent to some command, expression, or set of commands and expressions in the semantics of a programming language that we define for solving basic algebraic problems. 

The categorization task will be based on a semantic representation in a vector space model, using fasttext \citep{bojanowski2016enriching}. Each command or expression, will then have its arguments filled by one of the syntactic dependents of the verb, which will then map to the arguments in the described in the semantics of our target language. The categorization parameters are learned over the same data that \citep{Hosseini2014} use, and will be assessed in order to compare results directly. The target language will be the simple \IMP language.